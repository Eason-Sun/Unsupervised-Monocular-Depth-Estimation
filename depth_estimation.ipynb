{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "cs684_project7.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2xDbhOWBEcnI",
        "colab_type": "text"
      },
      "source": [
        "# Unsupervised Monocular Depth Estimation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "j55Huy1DpbM-"
      },
      "source": [
        "# Abstract\n",
        "The ultimate goal of this project is to produce a depth map given a single input image. The vast majority of attempts approach this in a supervised way, which requires ground truth depth information in the training phase. The downside is that the depth data is not only expensive to collect due to the complicated hardware requirements but also imprecise under various lightning conditions that are inevitable in the natural scene.\n",
        "\n",
        "\"Unsupervised Monocular Depth Estimation with Left-Right Consistency\" https://arxiv.org/pdf/1609.03677.pdf by  Clément Godard, Oisin Mac Aodha and Gabriel J. Brostow posed the depth estimation as an image reconstruction problem, which bypasses the needs for the depth information. They achieved this by proposing a fully connected deep neural network, and minimizing loss in appearance matching, disparity smoothness and left-right disparity consistency.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "aBUnJVF78C0U"
      },
      "source": [
        "# Introduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "JgonEHS07_g_"
      },
      "source": [
        "## Network Architecture at High Level\n",
        "\n",
        "![alt text](https://drive.google.com/uc?id=1bz8aXhrg9VzrWpje2Dyw4AimVqsREKWs)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "sHWQcE5Y8IAD"
      },
      "source": [
        "The training dataset is composed of calibrated stereo images. The left images are denoted as $I^l$, and right images are denoted as $I^r$.\n",
        "\n",
        "The naïve way is to feed the left image into the fully connected deep neural network which tries to learn the disparity mapping of right image from its left counterpart, and the disparity matrix is denoted as $d^r$. Then, by sampling the left image according to $d^r$, we should synthesize the right image, denoted as ${\\tilde{I}}^r$. Finally, we compare with the ground truth right image $I^r$ and backpropagate the loss to update network parameters.\n",
        "\n",
        "As a monocular depth estimation, the output disparity should be aligned with the single input image so that the naïve approach is not applicable in this case. No LR approach solves this problem by letting the network to learn the disparity map of left image from the right one. In this way, we can recover the left input image by sampling the right input image and compare with the ground truth.\n",
        "\n",
        "The architecture can be further refined by training the network with the left image so that it produces both disparity maps which then can be compared with stereo pairs. This approach mitigates the “text-copy” artifact from the No LR approach and significantly boosts the result.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "zJru46Ow9K3I"
      },
      "source": [
        " # The detailed of network structure\n",
        "## Use Yellow Block stands for layers of convolution layers in a resblock_basic\n",
        "![alt text](https://drive.google.com/uc?id=0Bwv6v6bgdX__UUVvWG5lblF2dU1sa2ZLYTZvWE52VU8yM2FF)\n",
        "## Use Black Block stands for layers of convolution layers in a resblock_basic\n",
        "![alt text](https://drive.google.com/uc?id=0Bwv6v6bgdX__a2trdGZtRUtxRHhEUU1xSkF0OGlBaDlDNUpv)\n",
        "## The Overall Neural Network Structure\n",
        "![alt text](https://drive.google.com/uc?id=0Bwv6v6bgdX__XzF5aW1sNGVqd01PajdKenNHOG1yWUlPb05R)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "EZsEjejO8LUG"
      },
      "source": [
        "## Image Sampler\n",
        "To synthesize the estimated image from the target’s disparity. The model adopts the image sampler from spatial transformer network (STN). The motivation is that STN chooses bilinear sampling so that it is fully differentiable. \n",
        "\n",
        "The code block below demonstrates that, given a left image and disparity map of right image from the left image, we can use grid_sample() to reconstruct the right image. Suppose both left and right images are 3x4x1, and the foreground object on the left image has disparity of 1.\n",
        "\n",
        "First, we generate the constant flow field without any disparity applied. We can observe that, grid_sample() produces the identical left image. \n",
        "\n",
        "Then, we create a disparity map and add it to the flow_field along x-direction (For rectified stereo pairs, objects are shifted along epipolar lines that are parallel along x-direction). The output image is the reconstructed right image. Note that the disparity matrix here is equivalent but not equal to not the pixel-level disparity, but this is not an issuce since it is learned by the network. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "IMHN7WcK8gkn",
        "outputId": "7fe8fed4-44f7-4632-dd3e-36c64736afb2",
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 474
        }
      },
      "source": [
        "import warnings\n",
        "warnings.simplefilter(\"ignore\")\n",
        "import torch\n",
        "\n",
        "width=4\n",
        "height=3\n",
        "left_image = torch.zeros(12).view(3,4)\n",
        "left_image[:,2]=5\n",
        "left_image = left_image.view(1,1,3,4).float()\n",
        "print(f\"Example left input image:\\n{left_image}\")\n",
        "\n",
        "rows = torch.linspace(0, 1, height)\n",
        "cols = torch.linspace(0, 1, width)\n",
        "y, x = torch.meshgrid(rows, cols)\n",
        "\n",
        "print(f'\\nConstruct a constant flow field in x direction:\\n{x}')\n",
        "flow = torch.stack((x,y), dim=2).unsqueeze(0)\n",
        "output_image_1 = torch.nn.functional.grid_sample(left_image, 2*flow-1, mode='bilinear', padding_mode='zeros')\n",
        "print(f'\\nThe output image should look identical to the left input image:\\n{output_image_1}')\n",
        "\n",
        "print('----------------------------------------------------------------')\n",
        "\n",
        "# Disparity (trainable) is the output from the network\n",
        "disparity = torch.zeros(12).view(3,4)\n",
        "disparity[:,1]=1/3\n",
        "disparity[:,2]=-2/3\n",
        "\n",
        "print(f'\\nFlow field after apply disparity map:\\n{x + disparity}')\n",
        "flow_with_disparity = torch.stack((x + disparity, y), dim=2).unsqueeze(0)\n",
        "pred_right_image = torch.nn.functional.grid_sample(left_image, 2*flow_with_disparity-1, mode='bilinear', padding_mode='zeros')\n",
        "print(f'\\nThe output image is the reconstructed right image by shifting the object in the left image to the left by one pixel:\\n{pred_right_image}\\n')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Example left input image:\n",
            "tensor([[[[0., 0., 5., 0.],\n",
            "          [0., 0., 5., 0.],\n",
            "          [0., 0., 5., 0.]]]])\n",
            "\n",
            "Construct a constant flow field in x direction:\n",
            "tensor([[0.0000, 0.3333, 0.6667, 1.0000],\n",
            "        [0.0000, 0.3333, 0.6667, 1.0000],\n",
            "        [0.0000, 0.3333, 0.6667, 1.0000]])\n",
            "\n",
            "The output image should look identical to the left input image:\n",
            "tensor([[[[0., 0., 5., 0.],\n",
            "          [0., 0., 5., 0.],\n",
            "          [0., 0., 5., 0.]]]])\n",
            "----------------------------------------------------------------\n",
            "\n",
            "Flow field after apply disparity map:\n",
            "tensor([[0.0000, 0.6667, 0.0000, 1.0000],\n",
            "        [0.0000, 0.6667, 0.0000, 1.0000],\n",
            "        [0.0000, 0.6667, 0.0000, 1.0000]])\n",
            "\n",
            "The output image is the reconstructed right image by shifting the object in the left image to the left by one pixel:\n",
            "tensor([[[[0., 5., 0., 0.],\n",
            "          [0., 5., 0., 0.],\n",
            "          [0., 5., 0., 0.]]]])\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "vLVuoJzP8lwm"
      },
      "source": [
        "## Loss Functions\n",
        "\n",
        "### 1) Appearance Matching Loss:\n",
        "Appearance matching loss is the weighted sum of the L1 loss and single scale structural similarity index measure (SSIM) loss.\n",
        "\n",
        "$$SSIM(x,y)=\\frac{(2\\mu_x\\mu_y+C_1)(2\\sigma_{xy}+C_2)}{({\\mu_x}^2+{\\mu_y}^2+C_1)({\\sigma_x}^2+{\\sigma_y}^2+C_2)}$$, \n",
        "\n",
        "where $C_1=(0.1L)^2$, $C_1=(0.3L)^2$ and L is the dynamic range (e.g. $L=1$). Note that the above means, variances and covariances are computed locally using a sliding $3\\times3$ kernel by *torch.nn.AvgPool2d(kernel_size=3, stride=1)*.\n",
        "\n",
        "$$L_1(x,y)=|x-y|$$\n",
        "\n",
        "As a result, the photometric image reconstruction loss $C^l_{ap}$ between the left image $I^l$ and the synthesized left image ${\\tilde{I}}^l$ is (weight of SSIM is set to 0.85),\n",
        "\n",
        "$$C^l_{ap}=\\frac{1}{N}\\sum_{i,j}{\\alpha\\frac{1-SSIM(I^l_{ij},{\\tilde{I}}^l_{ij})}{2}+(1-\\alpha)\\lVert I^l_{ij}-{\\tilde{I}}^l_{ij}\\rVert)}$$\n",
        "\n",
        "\n",
        "### 2) Disparity Smoothness Loss:\n",
        "The disparity smoothness loss serves as the regularization term which penalizes disparity discontinuity over photo-consistent area of the image and encourages disparity change at image boundaries. It ensures that pixels belonging to the same object would have same disparity.\n",
        "\n",
        "The implementation uses the bounded Gaussian kernel as the loss function. The image gradients and disparity gradients are calculated along both directions. The disparity smoothness loss of the left image is denoted as $C^l_{ds}$,\n",
        "\n",
        "$$C^l_{ds}=\\frac{1}{N}\\sum_{i,j}{|\\partial_xd^l_{ij}|e^{-\\lVert \\partial_xI^l_{i,j}\\rVert}+|\\partial_yd^l_{ij}|e^{-\\lVert \\partial_yI^l_{i,j}\\rVert}}$$\n",
        "\n",
        "As the formula shown, as the image gradient decrease (same object or constant background), increase of disparity gradient would incur higher smoothness loss. This penalty weight is bounded by 1 ($\\partial_xd^l_{ij}\\rightarrow 0, e^{-\\lVert \\partial_xI^l_{i,j}\\rVert}\\rightarrow 1$).\n",
        "\n",
        "On the contrary, when it detects a large jump in pixel values (e.g. boundaries), the penalty weight decreases accordingly. To the extreme degree, as $\\partial_xd^l_{ij}\\rightarrow \\infty, e^{-\\lVert \\partial_xI^l_{i,j}\\rVert}\\rightarrow 0$. In this case, change in disparity of any value won't incur any cost.\n",
        "\n",
        "\n",
        "### 3) Left-Right Disparity Consistency Loss:\n",
        "The final loss ensures the coherence between the left and right disparity maps, that is, given the right disparity map, by applying a left disparity transforms, we can recover the same left disparity map:\n",
        "\n",
        "$$C^l_{lr} = \\frac{1}{N}\\sum_{i,j}{d^l_{ij} -d^r_{ij+d^l_{ij}}}$$\n",
        "\n",
        "For the implementation part, we reused the image sampler for the appearance matching loss. Instead of using image as input, we use disparity matrix.\n",
        "\n",
        "\n",
        "### 4) Total Loss:\n",
        "All the cost above has the right mirrored counterparts. The total loss is defined below:\n",
        "\n",
        "$$C_s=\\alpha_ap(C^l_{ap}+C^r_{ap})+\\alpha_{ds}(C^l_{ds}+C^r_{ds})+\\alpha_{lr}(C^l_{lr}+C^r_{lr})$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "NsnzTVDAv0GM"
      },
      "source": [
        "\n",
        "## Dataset\n",
        "In this Project the datasets we choose is the kitti dataset can be download from http://www.cvlibs.net/datasets/kitti/raw_data.php\n",
        "or it can be extract by the code below.\n",
        "The datasets used for training is located in \"dataset/train\" and the datasets used for validation is located in \"dataset/validation\". \n",
        "The model use the images under \"dataset/train/*/image_02/data\" as left image and the images under \"dataset/train/*/image_03/data\" as right image"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "K_PEBunDsfNl",
        "colab": {}
      },
      "source": [
        "# !wget -i kitti_archives_to_download.txt -P .\n",
        "!wget -i kitti_train.txt -P dataset/train\n",
        "!wget -i kitti_validation.txt -P dataset/validation"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "yeauvdLfsnGE",
        "colab": {}
      },
      "source": [
        "\n",
        "!unzip /content/dataset/train/\\*.zip -d dataset/train\n",
        "!unzip /content/dataset/validation/\\*.zip -d dataset/validation"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "HvPP43BIxyGJ",
        "colab": {}
      },
      "source": [
        "!mv /content/dataset/train/*/* /content/dataset/train\n",
        "!mv /content/dataset/validation/*/* /content/dataset/validation"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "8GXQEIfXB_Qt"
      },
      "source": [
        "# Contribution\n",
        "Abstract: Yixing Sun\n",
        "\n",
        "Introduction: Yixing Sun, Jiaheng Zhao\n",
        "\n",
        "Code: Yixing Sun, Jiaheng Zhao\n",
        "\n",
        "Conclusion: Yixing Sun, Jiaheng Zhao\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "mUnqw7121EMS"
      },
      "source": [
        "## Data Transformations and Augmentations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "FLS0Qld81Dfg",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "import numpy as np\n",
        "\n",
        "def apply_transformations(mode='train'):\n",
        "    if mode == 'train':\n",
        "        data_transform = transforms.Compose([\n",
        "            ResizeImage(train=True),\n",
        "            RandomFlip(),\n",
        "            ToTensor(train=True),\n",
        "            AugmentImagePair()\n",
        "        ])\n",
        "        return data_transform\n",
        "    elif mode == 'test':\n",
        "        data_transform = transforms.Compose([\n",
        "            ResizeImage(train=False),\n",
        "            ToTensor(train=False),\n",
        "            DoTest(),\n",
        "        ])\n",
        "        return data_transform\n",
        "    else:\n",
        "        print('Wrong mode')\n",
        "\n",
        "\n",
        "class ResizeImage(object):\n",
        "    def __init__(self, train=True, size=(256, 512)):\n",
        "        self.train = train\n",
        "        self.transform = transforms.Resize(size)\n",
        "\n",
        "    def __call__(self, sample):\n",
        "        if self.train:\n",
        "            left_image = sample['left_image']\n",
        "            right_image = sample['right_image']\n",
        "            new_right_image = self.transform(right_image)\n",
        "            new_left_image = self.transform(left_image)\n",
        "            sample = {'left_image': new_left_image, 'right_image': new_right_image}\n",
        "        else:\n",
        "            left_image = sample\n",
        "            new_left_image = self.transform(left_image)\n",
        "            sample = new_left_image\n",
        "        return sample\n",
        "\n",
        "\n",
        "class DoTest(object):\n",
        "    def __call__(self, sample):\n",
        "        new_sample = torch.stack((sample, torch.flip(sample, [2])))\n",
        "        return new_sample\n",
        "\n",
        "\n",
        "class ToTensor(object):\n",
        "    def __init__(self, train):\n",
        "        self.train = train\n",
        "        self.transform = transforms.ToTensor()\n",
        "\n",
        "    def __call__(self, sample):\n",
        "        if self.train:\n",
        "            left_image = sample['left_image']\n",
        "            right_image = sample['right_image']\n",
        "            new_right_image = self.transform(right_image)\n",
        "            new_left_image = self.transform(left_image)\n",
        "            sample = {'left_image': new_left_image,\n",
        "                      'right_image': new_right_image}\n",
        "        else:\n",
        "            left_image = sample\n",
        "            sample = self.transform(left_image)\n",
        "        return sample\n",
        "\n",
        "\n",
        "class RandomFlip(object):\n",
        "    def __init__(self, do_augmentation=True):\n",
        "        self.transform = transforms.RandomHorizontalFlip(p=1)\n",
        "        self.do_augmentation = do_augmentation\n",
        "\n",
        "    def __call__(self, sample):\n",
        "        left_image = sample['left_image']\n",
        "        right_image = sample['right_image']\n",
        "        k = np.random.uniform(0, 1, 1)\n",
        "        if self.do_augmentation:\n",
        "            if k > 0.5:\n",
        "                fliped_left = self.transform(right_image)\n",
        "                fliped_right = self.transform(left_image)\n",
        "                sample = {'left_image': fliped_left, 'right_image': fliped_right}\n",
        "        else:\n",
        "            sample = {'left_image': left_image, 'right_image': right_image}\n",
        "        return sample\n",
        "\n",
        "\n",
        "class AugmentImagePair(object):\n",
        "    def __init__(self, augment_parameters=[0.8, 1.2, 0.5, 2.0, 0.8, 1.2], do_augmentation=True):\n",
        "        self.do_augmentation = do_augmentation\n",
        "        self.gamma_low = augment_parameters[0]  # 0.8\n",
        "        self.gamma_high = augment_parameters[1]  # 1.2\n",
        "        self.brightness_low = augment_parameters[2]  # 0.5\n",
        "        self.brightness_high = augment_parameters[3]  # 2.0\n",
        "        self.color_low = augment_parameters[4]  # 0.8\n",
        "        self.color_high = augment_parameters[5]  # 1.2\n",
        "\n",
        "    def __call__(self, sample):\n",
        "        left_image = sample['left_image']\n",
        "        right_image = sample['right_image']\n",
        "        p = np.random.uniform(0, 1, 1)\n",
        "        if self.do_augmentation:\n",
        "            if p > 0.5:\n",
        "                # randomly shift gamma\n",
        "                random_gamma = np.random.uniform(self.gamma_low, self.gamma_high)\n",
        "                left_image_aug = left_image ** random_gamma\n",
        "                right_image_aug = right_image ** random_gamma\n",
        "\n",
        "                # randomly shift brightness\n",
        "                random_brightness = np.random.uniform(self.brightness_low, self.brightness_high)\n",
        "                left_image_aug = left_image_aug * random_brightness\n",
        "                right_image_aug = right_image_aug * random_brightness\n",
        "\n",
        "                # randomly shift color\n",
        "                random_colors = np.random.uniform(self.color_low, self.color_high, 3)\n",
        "                for i in range(3):\n",
        "                    left_image_aug[i, :, :] *= random_colors[i]\n",
        "                    right_image_aug[i, :, :] *= random_colors[i]\n",
        "                # saturate\n",
        "                left_image_aug = torch.clamp(left_image_aug, 0, 1)\n",
        "                right_image_aug = torch.clamp(right_image_aug, 0, 1)\n",
        "\n",
        "                sample = {'left_image': left_image_aug, 'right_image': right_image_aug}\n",
        "\n",
        "        else:\n",
        "            sample = {'left_image': left_image, 'right_image': right_image}\n",
        "        return sample"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "GCjUNn3zucsb"
      },
      "source": [
        "## Data Loader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "GMmOFJTWuNsR",
        "colab": {}
      },
      "source": [
        "import os\n",
        "from PIL import Image\n",
        "from torch.utils.data import Dataset, DataLoader, ConcatDataset\n",
        "\n",
        "class Loader(Dataset):\n",
        "    def __init__(self, dir, mode='train', transform=None):\n",
        "        left_dir = os.path.join(dir, 'image_02/data/')\n",
        "        self.left_paths = sorted([os.path.join(left_dir, fname) for fname in os.listdir(left_dir)])\n",
        "        if mode == 'train':\n",
        "            right_dir = os.path.join(dir, 'image_03/data/')\n",
        "            self.right_paths = sorted([os.path.join(right_dir, fname) for fname in os.listdir(right_dir)])\n",
        "        self.transform = transform\n",
        "        self.mode = mode\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.left_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        left_image = Image.open(self.left_paths[idx])\n",
        "        if self.mode == 'train':\n",
        "            right_image = Image.open(self.right_paths[idx])\n",
        "            sample = {'left_image': left_image, 'right_image': right_image}\n",
        "\n",
        "            if self.transform:\n",
        "                sample = self.transform(sample)\n",
        "                return sample\n",
        "            else:\n",
        "                return sample\n",
        "        else:\n",
        "            if self.transform:\n",
        "                left_image = self.transform(left_image)\n",
        "            return left_image\n",
        "\n",
        "def create_dataloader(data_directory, mode, batch_size):\n",
        "    data_dirs = os.listdir(data_directory)\n",
        "    data_transform = apply_transformations(mode=mode)\n",
        "    datasets = [Loader(os.path.join(data_directory,\n",
        "                            data_dir), mode, transform=data_transform)\n",
        "                            for data_dir in data_dirs]\n",
        "    dataset = ConcatDataset(datasets)\n",
        "    n_img = len(dataset)\n",
        "    print('Use a dataset with', n_img, 'images')\n",
        "    if mode == 'train':\n",
        "        loader = DataLoader(dataset, batch_size=batch_size,\n",
        "                            shuffle=True, num_workers=8,\n",
        "                            pin_memory=True)\n",
        "    else:\n",
        "        loader = DataLoader(dataset, batch_size=batch_size,\n",
        "                            shuffle=False, num_workers=8,\n",
        "                            pin_memory=True)\n",
        "    return n_img, loader"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "KQfO6RtG50u7"
      },
      "source": [
        "## Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "U3cGbDd5w-7m",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class conv(nn.Module):\n",
        "    def __init__(self, num_in_layers, num_out_layers, kernel_size, stride):\n",
        "        super(conv, self).__init__()\n",
        "        self.kernel_size = kernel_size\n",
        "        self.conv_base = nn.Conv2d(num_in_layers, num_out_layers, kernel_size=kernel_size, stride=stride)\n",
        "        self.normalize = nn.BatchNorm2d(num_out_layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        p = int(np.floor((self.kernel_size-1)/2))\n",
        "        p2d = (p, p, p, p)\n",
        "        x = self.conv_base(F.pad(x, p2d))\n",
        "        x = self.normalize(x)\n",
        "        return F.elu(x, inplace=True)\n",
        "\n",
        "class maxpool(nn.Module):\n",
        "\n",
        "    def __init__(self, kernel_size):\n",
        "        super(maxpool, self).__init__()\n",
        "        self.kernel_size = kernel_size\n",
        "\n",
        "    def forward(self, x):\n",
        "        p = int(np.floor((self.kernel_size-1) / 2))\n",
        "        p2d = (p, p, p, p)\n",
        "        return F.max_pool2d(F.pad(x, p2d), self.kernel_size, stride=2)\n",
        "\n",
        "class resconv_basic(nn.Module):\n",
        "    def __init__(self, num_in_layers, num_out_layers, kernel_size, stride):\n",
        "        super(resconv_basic, self).__init__()\n",
        "        self.num_out_layers = num_out_layers\n",
        "        self.conv1 = conv(num_in_layers, num_out_layers, kernel_size, stride)\n",
        "        self.conv2 = conv(num_out_layers, num_out_layers, 3, 1)\n",
        "        self.conv3 = nn.Conv2d(num_in_layers, num_out_layers, kernel_size=1, stride=stride)\n",
        "        self.normalize = nn.BatchNorm2d(num_out_layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x_out = self.conv1(x)\n",
        "        x_out = self.conv2(x_out)\n",
        "        shortcut = self.conv3(x)\n",
        "        return F.elu(self.normalize(x_out + shortcut), inplace=True)\n",
        "\n",
        "def resblock_basic(num_in_layers, num_out_layers, num_blocks, stride):\n",
        "    layers = []\n",
        "    layers.append(resconv_basic(num_in_layers, num_out_layers, 7, stride))\n",
        "    for i in range(1, num_blocks):\n",
        "        layers.append(resconv_basic(num_out_layers, num_out_layers, 3, 1))\n",
        "    return nn.Sequential(*layers)\n",
        "\n",
        "class upconv(nn.Module):\n",
        "    def __init__(self, num_in_layers, num_out_layers, kernel_size, scale):\n",
        "        super(upconv, self).__init__()\n",
        "        self.scale = scale\n",
        "        self.conv1 = conv(num_in_layers, num_out_layers, kernel_size, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = nn.functional.interpolate(x, scale_factor=self.scale, mode='bilinear', align_corners=True)\n",
        "        return self.conv1(x)\n",
        "\n",
        "\n",
        "class get_disp(nn.Module):\n",
        "    def __init__(self, num_in_layers):\n",
        "        super(get_disp, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(num_in_layers, 2, kernel_size=3, stride=1)\n",
        "        self.normalize = nn.BatchNorm2d(2)\n",
        "        self.sigmoid = torch.nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        p = 1\n",
        "        p2d = (p, p, p, p)\n",
        "        x = self.conv1(F.pad(x, p2d))\n",
        "        x = self.normalize(x)\n",
        "        return 0.3 * self.sigmoid(x)\n",
        "\n",
        "class Depth_Est_Model(nn.Module):\n",
        "    def __init__(self, num_in_layers):\n",
        "        super(Depth_Est_Model, self).__init__()\n",
        "        # encoder\n",
        "        self.conv1 = conv(num_in_layers, 64, 7, 2)\n",
        "        self.pool1 = maxpool(3)  # H/4  -   64D\n",
        "        self.conv2 = resblock_basic(64, 128, 3, 4)\n",
        "        self.conv3 = resblock_basic(128, 256, 3, 4)\n",
        "\n",
        "\n",
        "        # decoder\n",
        "        self.upconv4 = upconv(256, 128, 3, 4)\n",
        "        self.iconv4 = conv(128 + 128, 128, 3, 1)\n",
        "\n",
        "        self.upconv3 = upconv(128, 64, 3, 4)\n",
        "        self.iconv3 = conv(64 + 64, 64, 3, 1)\n",
        "\n",
        "        self.upconv2 = upconv(64, 32, 3, 2)\n",
        "        self.iconv2 = conv(32 + 64, 32, 3, 1)\n",
        "        self.disp2_layer = get_disp(32)\n",
        "\n",
        "        self.upconv1 = upconv(32, 16, 3, 2)\n",
        "        self.iconv1 = conv(16 + 2, 16, 3, 1)\n",
        "        self.disp1_layer = get_disp(16)\n",
        "\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.xavier_uniform_(m.weight)\n",
        "\n",
        "    def forward(self, x): # (8, 3, 256, 512)\n",
        "        # encoder\n",
        "        \n",
        "        x1 = self.conv1(x)  # (8, 64, 128 256)\n",
        "        x_pool1 = self.pool1(x1)  # (8, 64, 64, 128)\n",
        "        x2 = self.conv2(x_pool1)  # (8, 128, 16, 32)\n",
        "        x3 = self.conv3(x2)  # (8, 256, 4, 8)\n",
        "        # skips\n",
        "        skip1 = x1  # (8, 64, 128 256)\n",
        "        skip2 = x_pool1  # (8, 64, 64, 128)\n",
        "        skip3 = x2  # (8, 128, 16, 32)\n",
        "\n",
        "        # decoder\n",
        "        upconv4 = self.upconv4(x3)  # (8, 128, 16, 32)\n",
        "        concat4 = torch.cat((upconv4, skip3), 1)  # (8, 256, 16, 32)\n",
        "        iconv4 = self.iconv4(concat4)  # (8, 128, 16, 32)\n",
        "\n",
        "        upconv3 = self.upconv3(iconv4)  # (8, 64, 64, 128)\n",
        "        concat3 = torch.cat((upconv3, skip2), 1)  # (8, 128, 64, 128)\n",
        "        iconv3 = self.iconv3(concat3)  # (8, 64, 64, 128)\n",
        "\n",
        "        upconv2 = self.upconv2(iconv3)  # (8, 32, 128, 256)\n",
        "        concat2 = torch.cat((upconv2, skip1), 1)  # (8, 96, 128, 256)\n",
        "        iconv2 = self.iconv2(concat2)  # (8, 32, 128, 256)\n",
        "        self.disp2 = self.disp2_layer(iconv2) # (8, 2, 128, 256)\n",
        "        self.udisp2 = nn.functional.interpolate(self.disp2, scale_factor=2, mode='bilinear', align_corners=True)\n",
        "\n",
        "        upconv1 = self.upconv1(iconv2)  # (8, 16, 256, 512)\n",
        "        concat1 = torch.cat((upconv1, self.udisp2), 1)  # (8, 18, 256, 512)\n",
        "        iconv1 = self.iconv1(concat1)  # (8, 16, 128, 256)\n",
        "        self.disp1 = self.disp1_layer(iconv1)  # (8, 2, 256, 512)\n",
        "\n",
        "        return self.disp1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "LELZf6do6BZV"
      },
      "source": [
        "## Loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "8rgUtqxk6Em1",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class Loss(nn.modules.Module):\n",
        "\n",
        "    def __init__(self, structural_similarity_w=0.85, disp_smoothness_w=0.1, lr_disp_consistency_w=1.0):\n",
        "        super(Loss, self).__init__()\n",
        "        self.structural_similarity_w = structural_similarity_w\n",
        "        self.disp_smoothness_w = disp_smoothness_w\n",
        "        self.lr_disp_consistency_w = lr_disp_consistency_w\n",
        "\n",
        "    # Formula to calculate the ssim:\n",
        "    # [(2 * mu_x * mu_y + c_1) * (2 * cov_x_y + c_2)] /\n",
        "    # [(mu_x^2 + mu_y^2 + c_1) * (sigma_x^2 + sigma_y^2 + c_2)]\n",
        "    def structural_similarity(self, pred, target, dr=1, kernel_size=3):\n",
        "        c_1 = (0.01 * dr) ** 2\n",
        "        c_2 = (0.03 * dr) ** 2\n",
        "        # Calculate the local mean of pred and target with kernel of size 3\n",
        "        pred_mean = nn.AvgPool2d(kernel_size=kernel_size, stride=1)(pred)\n",
        "        target_mean = nn.AvgPool2d(kernel_size=kernel_size, stride=1)(target)\n",
        "        pred_mean_square = pred_mean.pow(2)\n",
        "        target_mean_square = target_mean.pow(2)\n",
        "        pred_target_mean = pred_mean * target_mean\n",
        "        # Calculate their variance and covariance\n",
        "        pred_variance = nn.AvgPool2d(kernel_size=kernel_size, stride=1)(pred * pred) - pred_mean_square\n",
        "        target_variance = nn.AvgPool2d(kernel_size=kernel_size, stride=1)(target * target) - target_mean_square\n",
        "        covariance = nn.AvgPool2d(kernel_size=kernel_size, stride=1)(pred * target) - pred_target_mean\n",
        "        n = (2 * pred_target_mean + c_1) * (2 * covariance + c_2)\n",
        "        d = (pred_mean_square + target_mean_square + c_1) * (pred_variance + target_variance + c_2)\n",
        "        ssim_norm = torch.clamp((1 - n / d) / 2, 0, 1)\n",
        "        # return the global mean of all local ssim\n",
        "        return torch.mean(ssim_norm)\n",
        "\n",
        "    # Use bounded Gaussian loss for regularization.\n",
        "    def disp_smoothness(self, disp_map, target_image):\n",
        "        disp_grad_x, disp_grad_y = self.gradient(disp_map)\n",
        "        img_grad_x, img_grad_y = self.gradient(target_image)\n",
        "        # Penalty for disparity discontinuity is large in photo-consistent area, but bounded by exp(0)\n",
        "        # Penalty for disparity discontinuity is small when experiencing large gradient jumps, bounded by exp(-inf)\n",
        "        penalty_weight_x = torch.exp(-torch.mean(torch.abs(img_grad_x), 1, keepdim=True))\n",
        "        penalty_weight_y = torch.exp(-torch.mean(torch.abs(img_grad_y), 1, keepdim=True))\n",
        "        smoothness_loss_x = torch.abs(disp_grad_x * penalty_weight_x)\n",
        "        smoothness_loss_y = torch.abs(disp_grad_y * penalty_weight_y)\n",
        "        # Total smoothness loss is the sum of loss along two directions.\n",
        "        return torch.mean(smoothness_loss_x + smoothness_loss_y)\n",
        "\n",
        "    def lr_disp_consistency(self, pred_disp_l, pred_disp_r):\n",
        "        r_to_l_disp = self.generate_image(pred_disp_r, pred_disp_l, True)\n",
        "        l_to_r_disp = self.generate_image(pred_disp_l, pred_disp_r, False)\n",
        "        r_to_l_loss = torch.mean(torch.abs(r_to_l_disp - pred_disp_l))\n",
        "        l_to_r_loss = torch.mean(torch.abs(l_to_r_disp - pred_disp_r))\n",
        "        return r_to_l_loss + l_to_r_loss\n",
        "\n",
        "    def gradient(self, img):  # img: (num_batch, num_channel, height, width)\n",
        "        # Use padding to keep the shape consistent\n",
        "        img_padding_right = F.pad(img, (0, 1, 0, 0), mode=\"replicate\")\n",
        "        img_padding_bottom = F.pad(img, (0, 0, 0, 1), mode=\"replicate\")\n",
        "        grad_x = img_padding_right[:, :, :, :-1] - img_padding_right[:, :, :, 1:]\n",
        "        grad_y = img_padding_bottom[:, :, :-1, :] - img_padding_bottom[:, :, 1:, :]\n",
        "        return grad_x, grad_y\n",
        "\n",
        "    def generate_image(self, img, disp, is_left):\n",
        "        if is_left: disp = -disp\n",
        "        batch_size, _, height, width = img.size()\n",
        "        rows = torch.linspace(0, 1, height).type_as(img)\n",
        "        cols = torch.linspace(0, 1, width).type_as(img)\n",
        "        y, x = torch.meshgrid(rows, cols)\n",
        "        delta_x = disp[:, 0, :, :]\n",
        "        x = x.repeat(batch_size, 1, 1)\n",
        "        y = y.repeat(batch_size, 1, 1)\n",
        "        flow_field = torch.stack((x + delta_x, y), dim=3)\n",
        "        return F.grid_sample(img, 2*flow_field - 1)\n",
        "\n",
        "    def forward(self, pred_disp, target_img):\n",
        "        self.pred_disp_l = pred_disp[:, 0, :, :].unsqueeze(1)\n",
        "        self.pred_disp_r = pred_disp[:, 1, :, :].unsqueeze(1)\n",
        "        target_img_l, target_img_r = target_img\n",
        "        self.pred_img_l = self.generate_image(target_img_r, self.pred_disp_l, True)\n",
        "        self.pred_img_r = self.generate_image(target_img_l, self.pred_disp_r, False)\n",
        "\n",
        "        # Appearance Matching Loss\n",
        "        L1_l = torch.mean(torch.abs(target_img_l - self.pred_img_l))\n",
        "        L1_r = torch.mean(torch.abs(target_img_r - self.pred_img_r))\n",
        "        ssim_l = self.structural_similarity(self.pred_img_l, target_img_l)\n",
        "        ssim_r = self.structural_similarity(self.pred_img_r, target_img_r)\n",
        "        appearance_matching_loss = self.structural_similarity_w * (ssim_l + ssim_r) + \\\n",
        "                                   (1 - self.structural_similarity_w) * (L1_l + L1_r)\n",
        "\n",
        "        # Disparity Smoothness Loss\n",
        "        disp_left_smoothness = self.disp_smoothness(self.pred_disp_l, target_img_l)\n",
        "        disp_right_smoothness = self.disp_smoothness(self.pred_disp_r, target_img_r)\n",
        "        disparity_smoothness_loss = disp_left_smoothness + disp_right_smoothness\n",
        "\n",
        "        # Left-Right Disparity Consistency Loss\n",
        "        l_r_disparity_consistency_loss = self.lr_disp_consistency(self.pred_disp_l, self.pred_disp_r)\n",
        "\n",
        "        loss = appearance_matching_loss + \\\n",
        "            self.disp_smoothness_w * disparity_smoothness_loss + \\\n",
        "            self.lr_disp_consistency_w * l_r_disparity_consistency_loss\n",
        "\n",
        "        return loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "-0VGVEl36IPa"
      },
      "source": [
        "## Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "A0-BzMZFtYsW",
        "colab": {}
      },
      "source": [
        "import collections\n",
        "\n",
        "def to_device(input, device='cuda:0'):\n",
        "    if torch.is_tensor(input):\n",
        "        return input.to(device=device)\n",
        "    elif isinstance(input, str):\n",
        "        return input\n",
        "    elif isinstance(input, collections.Mapping):\n",
        "        return {k: to_device(sample, device=device) for k, sample in input.items()}\n",
        "    elif isinstance(input, collections.Sequence):\n",
        "        return [to_device(sample, device=device) for sample in input]\n",
        "    else:\n",
        "        raise TypeError(f\"Input must contain tensor, dict or list, found {type(input)}\")\n",
        "\n",
        "def adjust_learning_rate(optimizer, epoch, learning_rate):\n",
        "    if epoch >= 30 and epoch < 40:\n",
        "        lr = learning_rate / 2\n",
        "    elif epoch >= 40:\n",
        "        lr = learning_rate / 4\n",
        "    else:\n",
        "        lr = learning_rate\n",
        "    for param_group in optimizer.param_groups:\n",
        "        param_group['lr'] = lr\n",
        "\n",
        "def post_process_disparity(disp):\n",
        "    (_, h, w) = disp.shape\n",
        "    l_disp = disp[0, :, :]\n",
        "    r_disp = np.fliplr(disp[1, :, :])\n",
        "    m_disp = 0.5 * (l_disp + r_disp)\n",
        "    (l, _) = np.meshgrid(np.linspace(0, 1, w), np.linspace(0, 1, h))\n",
        "    l_mask = 1.0 - np.clip(20 * (l - 0.05), 0, 1)\n",
        "    r_mask = np.fliplr(l_mask)\n",
        "    return r_mask * l_disp + l_mask * r_disp + (1.0 - l_mask - r_mask) * m_disp\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "5feIv_yqvKY1",
        "colab": {}
      },
      "source": [
        "import argparse\n",
        "import time\n",
        "import torch\n",
        "import numpy as np\n",
        "import torch.optim as optim\n",
        "\n",
        "class Model:\n",
        "\n",
        "    def __init__(self, args):\n",
        "        self.args = args\n",
        "\n",
        "        # Set up model\n",
        "        self.model = Depth_Est_Model(num_in_layers=3)\n",
        "        self.model = self.model.to('cuda:0')\n",
        "        if args.mode == 'train':\n",
        "            self.loss_function = Loss().to('cuda:0')\n",
        "            self.optimizer = optim.Adam(self.model.parameters(), lr=args.learning_rate)\n",
        "            # Load validation data\n",
        "            self.val_n_img, self.val_loader = create_dataloader(args.val_data_dir, args.mode, args.batch_size)\n",
        "        else:\n",
        "            self.model.load_state_dict(torch.load(args.model_path))\n",
        "            args.batch_size = 1\n",
        "\n",
        "        # Load training data\n",
        "        self.n_img, self.loader = create_dataloader(args.data_dir, args.mode, args.batch_size)\n",
        "        self.output_directory = args.output_directory\n",
        "        torch.cuda.synchronize()\n",
        "\n",
        "\n",
        "    def train(self):\n",
        "        losses = []\n",
        "        val_losses = []\n",
        "        best_loss = float('Inf')\n",
        "        best_val_loss = float('Inf')\n",
        "        running_val_loss = 0.0\n",
        "\n",
        "        # Compute the initial validation loss using the untrained network\n",
        "        self.model.eval()\n",
        "        for data in self.val_loader:\n",
        "            data = to_device(data)\n",
        "            left = data['left_image']\n",
        "            right = data['right_image']\n",
        "            disps = self.model(left)\n",
        "            loss = self.loss_function(disps, [left, right])\n",
        "            val_losses.append(loss.item())\n",
        "            running_val_loss += loss.item()\n",
        "        \n",
        "        # Compute the initial validation loss per batch\n",
        "        running_val_loss /= self.val_n_img / self.args.batch_size\n",
        "        print('Val_loss:', running_val_loss)\n",
        "\n",
        "        for epoch in range(self.args.epochs):\n",
        "            adjust_learning_rate(self.optimizer, epoch, self.args.learning_rate)\n",
        "            c_time = time.time()\n",
        "\n",
        "            # Training\n",
        "            running_loss = 0.0\n",
        "            self.model.train()\n",
        "            for data in self.loader:\n",
        "                # Load data\n",
        "                data = to_device(data)\n",
        "                left = data['left_image']\n",
        "                right = data['right_image']\n",
        "\n",
        "                # One optimization iteration\n",
        "                self.optimizer.zero_grad()\n",
        "                disps = self.model(left)\n",
        "                loss = self.loss_function(disps, [left, right])\n",
        "                loss.backward()\n",
        "                self.optimizer.step()\n",
        "                losses.append(loss.item())\n",
        "                running_loss += loss.item()\n",
        "\n",
        "            # Validation\n",
        "            running_val_loss = 0.0\n",
        "            self.model.eval()\n",
        "            for data in self.val_loader:\n",
        "                data = to_device(data)\n",
        "                left = data['left_image']\n",
        "                right = data['right_image']\n",
        "                disps = self.model(left)\n",
        "                loss = self.loss_function(disps, [left, right])\n",
        "                val_losses.append(loss.item())\n",
        "                running_val_loss += loss.item()\n",
        "\n",
        "            # Estimate loss per batch\n",
        "            running_loss /= self.n_img / self.args.batch_size\n",
        "            running_val_loss /= self.val_n_img / self.args.batch_size\n",
        "            print ('Epoch:', epoch + 1,\n",
        "                   'train_loss:', running_loss,\n",
        "                   'val_loss:', running_val_loss,\n",
        "                   'time:', round(time.time() - c_time, 3), 's')\n",
        "            self.save(self.args.model_path + '_last.pth')\n",
        "            if running_val_loss < best_val_loss:\n",
        "                self.save(self.args.model_path + '_cpt.pth')\n",
        "                best_val_loss = running_val_loss\n",
        "                print('Model_saved')\n",
        "\n",
        "        print ('Finished Training. Best loss:', best_loss)\n",
        "        self.save(self.args.model_path)\n",
        "\n",
        "    def save(self, path):\n",
        "        torch.save(self.model.state_dict(), path)\n",
        "\n",
        "    def load(self, path):\n",
        "        self.model.load_state_dict(torch.load(path))\n",
        "\n",
        "    def test(self):\n",
        "        self.model.eval()\n",
        "        disparities = np.zeros((self.n_img,\n",
        "                               256, 512),\n",
        "                               dtype=np.float32)\n",
        "        disparities_pp = np.zeros((self.n_img,\n",
        "                                  256, 512),\n",
        "                                  dtype=np.float32)\n",
        "        with torch.no_grad():\n",
        "            for (i, data) in enumerate(self.loader):\n",
        "                # Get the inputs\n",
        "                data = to_device(data)\n",
        "                left = data.squeeze()\n",
        "                # Do a forward pass\n",
        "                disps = self.model(left)\n",
        "                disp = disps[:, 0, :, :].unsqueeze(1)\n",
        "                disparities[i] = disp[0].squeeze().cpu().numpy()\n",
        "                disparities_pp[i] = \\\n",
        "                    post_process_disparity(disps[:, 0, :, :]\\\n",
        "                                           .cpu().numpy())\n",
        "        print(len(disparities_pp))\n",
        "        np.save(self.output_directory + '/disparities.npy', disparities)\n",
        "        np.save(self.output_directory + '/disparities_pp.npy', disparities_pp)\n",
        "        print('Finished Testing')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "WMsPr8311Sh0",
        "colab": {}
      },
      "source": [
        "from easydict import EasyDict as edict\n",
        "args = edict({'data_dir':'/content/dataset/train',\n",
        "              'val_data_dir':'/content/dataset/validation',\n",
        "              'model_path':'dataset',\n",
        "              'mode':'train',\n",
        "              'epochs':200,\n",
        "              'learning_rate':1e-4,\n",
        "              'batch_size': 8,\n",
        "              'output_directory':'testing_output/', \n",
        "              'input_channels': 3})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ly_9tnCk6UNc",
        "colab": {}
      },
      "source": [
        "# Train without pretrained model\n",
        "model = Model(args)\n",
        "model.train()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "d3qDAHdLUPj-",
        "outputId": "72da72f4-4455-4310-a23c-046f49c1480a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 337
        }
      },
      "source": [
        "# load losses from local\n",
        "import csv\n",
        "loss = []\n",
        "val_loss = []\n",
        "with open('losses.csv', 'r') as file:\n",
        "    reader = csv.reader(file)\n",
        "    for row in reader:\n",
        "      loss.append(float(row[0]))\n",
        "      val_loss.append(float(row[1]))\n",
        "        # loss.append((float)row))\n",
        "        # print(row)\n",
        "epochs = np.arange(len(loss))\n",
        "print(loss)\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(epochs,loss)\n",
        "plt.plot(epochs,val_loss)\n",
        "labels = [\"loss\",\"val_loss\"]\n",
        "plt.ylabel(\"loss\")\n",
        "plt.xlabel(\"epochs\")\n",
        "plt.legend(labels,loc = \"upper right\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.5833239185090606, 0.5392689951212867, 0.5139510468193659, 0.4929331028717114, 0.4752719935157623, 0.4589763726302253, 0.4442545501416949, 0.43157764834938483, 0.420018585449623, 0.40910318182581856, 0.39828727468694874, 0.38720173093634813, 0.37550568086725333, 0.3648246563998251, 0.3531749500996262, 0.3429895746035924, 0.3328861317844491, 0.32287958083688784, 0.3133964761384225, 0.30587294156170963, 0.2974440427377684, 0.29049167736902726, 0.28368572881359333, 0.27788344050742314, 0.2723226540038472, 0.26791213252267215, 0.2637390079239309, 0.25865318352352473, 0.254003650780717, 0.25041690521621424, 0.24217611447201254, 0.23962927319742153, 0.2380251001927033, 0.23602270951000995, 0.23440315171578563, 0.23334471651159208, 0.233448024915593, 0.23093077070438706, 0.22912661312633392, 0.22825988949267292, 0.22457494673821707, 0.22331278960184464, 0.22210210478248035]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7fa2aa5d40b8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "dVvnYMtKW46h",
        "outputId": "cd2cc33a-7c44-43a9-cff8-2cb1ae1754d6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 748
        }
      },
      "source": [
        "# print the loss history in the model\n",
        "for i in range(len(loss)):\n",
        "  print(\"epochs  {0} loss: {1}  val_loss  {2}\".format(i+1, loss[i], val_loss[i]))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epochs  1 loss: 0.5833239185090606  val_loss  0.5479436865750196\n",
            "epochs  2 loss: 0.5392689951212867  val_loss  0.525249026720492\n",
            "epochs  3 loss: 0.5139510468193659  val_loss  0.489516802676832\n",
            "epochs  4 loss: 0.4929331028717114  val_loss  0.45962558403488035\n",
            "epochs  5 loss: 0.4752719935157623  val_loss  0.45323426293023283\n",
            "epochs  6 loss: 0.4589763726302253  val_loss  0.4530073797634583\n",
            "epochs  7 loss: 0.4442545501416949  val_loss  0.429679864997403\n",
            "epochs  8 loss: 0.43157764834938483  val_loss  0.42204896947493314\n",
            "epochs  9 loss: 0.420018585449623  val_loss  0.40914464883567875\n",
            "epochs  10 loss: 0.40910318182581856  val_loss  0.395912802530108\n",
            "epochs  11 loss: 0.39828727468694874  val_loss  0.39436087741718423\n",
            "epochs  12 loss: 0.38720173093634813  val_loss  0.4051278235422754\n",
            "epochs  13 loss: 0.37550568086725333  val_loss  0.36081880084739243\n",
            "epochs  14 loss: 0.3648246563998251  val_loss  0.34594174781879344\n",
            "epochs  15 loss: 0.3531749500996262  val_loss  0.3454954899274386\n",
            "epochs  16 loss: 0.3429895746035924  val_loss  0.33400239629193773\n",
            "epochs  17 loss: 0.3328861317844491  val_loss  0.35917018586029614\n",
            "epochs  18 loss: 0.32287958083688784  val_loss  0.32641895068957405\n",
            "epochs  19 loss: 0.3133964761384225  val_loss  0.32117010110211236\n",
            "epochs  20 loss: 0.30587294156170963  val_loss  0.32055655748921447\n",
            "epochs  21 loss: 0.2974440427377684  val_loss  0.3108719061380424\n",
            "epochs  22 loss: 0.29049167736902726  val_loss  0.30578834223883666\n",
            "epochs  23 loss: 0.28368572881359333  val_loss  0.2928661424796898\n",
            "epochs  24 loss: 0.27788344050742314  val_loss  0.2962981925144062\n",
            "epochs  25 loss: 0.2723226540038472  val_loss  0.28790472232833153\n",
            "epochs  26 loss: 0.26791213252267215  val_loss  0.2946628388218719\n",
            "epochs  27 loss: 0.2637390079239309  val_loss  0.2934975465315584\n",
            "epochs  28 loss: 0.25865318352352473  val_loss  0.28585570277640265\n",
            "epochs  29 loss: 0.254003650780717  val_loss  0.2799354952683664\n",
            "epochs  30 loss: 0.25041690521621424  val_loss  0.2804306163123968\n",
            "epochs  31 loss: 0.24217611447201254  val_loss  0.27808154641028354\n",
            "epochs  32 loss: 0.23962927319742153  val_loss  0.27652513810349966\n",
            "epochs  33 loss: 0.2380251001927033  val_loss  0.2770234513494973\n",
            "epochs  34 loss: 0.23602270951000995  val_loss  0.2767609032717618\n",
            "epochs  35 loss: 0.23440315171578563  val_loss  0.2757098070012485\n",
            "epochs  36 loss: 0.23334471651159208  val_loss  0.27672349762477016\n",
            "epochs  37 loss: 0.233448024915593  val_loss  0.27756644856452334\n",
            "epochs  38 loss: 0.23093077070438706  val_loss  0.27541914834236536\n",
            "epochs  39 loss: 0.22912661312633392  val_loss  0.27583673780159973\n",
            "epochs  40 loss: 0.22825988949267292  val_loss  0.27446712920109484\n",
            "epochs  41 loss: 0.22457494673821707  val_loss  0.2728565654327213\n",
            "epochs  42 loss: 0.22331278960184464  val_loss  0.2745949627105703\n",
            "epochs  43 loss: 0.22210210478248035  val_loss  0.275499760119383\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "wo-5iJJt5rtj",
        "colab": {}
      },
      "source": [
        "# Train with previous best model\n",
        "# model = Model(args)\n",
        "# model.load('models/model_cpt.pth')\n",
        "# model.train()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Yeeo4u6P6vBA",
        "colab": {}
      },
      "source": [
        "test_args = edict({'data_dir':'test',\n",
        "                   'model_path':'models/model_cpt.pth', \n",
        "                   'output_directory':'test/2011_09_26_drive_0101_sync', \n",
        "                   'mode':'test'})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "7_BfGHWb6ltP",
        "outputId": "8e3193d5-613e-4598-d542-fd688546c740",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "# Test with previous best model\n",
        "model = Model(test_args)\n",
        "model.test()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Use a dataset with 936 images\n",
            "936\n",
            "Finished Testing\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "z57OXO4uOgMt",
        "outputId": "340ea905-4339-4210-c730-5ff670554233",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 350
        }
      },
      "source": [
        "import skimage.transform\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "disp = np.load('test/2011_09_26_drive_0101_sync/disparities_pp.npy')  # Or disparities.npy for output without post-processing\n",
        "disp.shape\n",
        "disp_to_img = skimage.transform.resize(disp[0].squeeze(), [375, 1242], mode='constant')\n",
        "plt.imshow(disp_to_img, cmap='plasma')\n",
        "plt.imsave(os.path.join(test_args.output_directory,\n",
        "                        test_args.model_path.split('/')[-1][:-4]+'_test_output.png'), disp_to_img, cmap='plasma')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "8RgMyc7qXhC5"
      },
      "source": [
        "## Conclusion\n",
        "\n",
        "### 1) Training Datasets:\n",
        "Only the partial kitti dataset is used to train our model. The total size of kitti dataset is 175GB which is neither possible nor necessary for this project. Therefore, the chosen training dataset is of size 110GB that is about 2/3 portion of the whole dataset. As a result, the overall accuracy still have the room for improvement.\n",
        "\n",
        "### 2) Loss\n",
        "Due to the time and computational contraints, our network was only trained for 43 epochs with learning rate of 0.0001. The loss graph shows the validation loss starts to converge around the 30th epoch, and the training loss still keeps declining, which is arguably the sign of overfitting. In comparison with the original model Godard et al. presented, we simplify their architecture so that our network only produces one pair of disparity maps instead of four. Consequently, we observed that, with the same amount of data and training epoches, our final loss (~ 0.2) is about one quarter of the origianl loss (~ 0.9).\n",
        "\n",
        "### 3) Prediction\n",
        "For demonstration purposes, We generated two gif files from our testing results (Please find them in your unzipped folder or open the Juypter notebook). Our model preforms well when there exists different objects at different depth levels in the scene, for instance, trees, street lights and traffic signs.\n",
        "\n",
        "From the second gif, We also observed that, the network could yield mispredictions in the photo-consistent areas where less occlusions are found. The sky tends to be considered as near oject somehow for its brighter color, whereas road that near the camera shows color in blue when referenced objects were absent. We may argue that parameter tuning in kernel size might mitigate this artifact since larger kernel could magnify the difference between patches, which leads to easy correspondence matching between stereo pairs.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "TKL9OGr9HfrA"
      },
      "source": [
        "# ![alt text](https://drive.google.com/uc?id=1Nq2Lk9ls8jN3-zOpZHMrUESLI8WNUYAp)\n",
        "![alt text](https://drive.google.com/uc?id=1HVQBDJ2b68KwCwRBeKX8yykqvmq8b7QC)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1IEUO61OiXWM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}